#!/usr/bin/env python3
"""
Phase 1: Data Pipeline & Splits Validation

This script validates:
1. Data loading (Adamson, K562, RPE1)
2. GEARS "simulation" split (global baseline)
3. LOGO split (functional class holdout)

Goal: Verify no data leakage and correct split logic.
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
import anndata as ad
import json

# Add src to path
base_dir = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(base_dir / "src"))

from goal_2_baselines.split_logic import load_split_config
from goal_2_baselines.baseline_runner import compute_pseudobulk_expression_changes

print("=" * 70)
print("Phase 1: Data Pipeline & Splits Validation")
print("=" * 70)
print()

# Define dataset paths
DATASETS = {
    "adamson": {
        "adata_path": base_dir.parent / "paper" / "benchmark" / "data" / "gears_pert_data" / "adamson" / "perturb_processed.h5ad",
        "split_path": base_dir / "results" / "goal_2_baselines" / "splits" / "adamson_split_seed1.json",
    },
    "k562": {
        "adata_path": Path("/Users/samuelminer/Documents/classes/nih_research/data_replogle_k562_essential/perturb_processed.h5ad"),
        "split_path": base_dir / "results" / "goal_2_baselines" / "splits" / "replogle_k562_essential_split_seed1.json",
    },
    "rpe1": {
        "adata_path": Path("/Users/samuelminer/Documents/classes/nih_research/data_replogle_rpe1_essential/perturb_processed.h5ad"),
        "split_path": base_dir / "results" / "goal_2_baselines" / "splits" / "replogle_rpe1_essential_split_seed1.json",
    },
}

ANNOTATION_PATHS = {
    "adamson": base_dir / "data" / "annotations" / "adamson_functional_classes_go.tsv",
    "k562": base_dir / "data" / "annotations" / "replogle_k562_functional_classes_go.tsv",
    "rpe1": base_dir / "data" / "annotations" / "replogle_rpe1_functional_classes_go.tsv",
}

# 1. Data Loading Verification
print("=" * 70)
print("1. DATA LOADING VERIFICATION")
print("=" * 70)
print()

dataset_info = {}

for name, paths in DATASETS.items():
    if not paths['adata_path'].exists():
        print(f"⚠️  Skipping {name}: data file not found at {paths['adata_path']}")
        continue
    
    print(f"\n{name.upper()}:")
    print(f"  Data: {paths['adata_path']}")
    
    # Load data
    try:
        adata = ad.read_h5ad(paths['adata_path'])
        print(f"  ✓ Loaded successfully")
    except Exception as e:
        print(f"  ✗ Failed to load: {e}")
        continue
    
    print(f"  Shape: {adata.shape} (cells × genes)")
    print(f"  Genes (vars): {len(adata.var)}")
    print(f"  Cells (obs): {len(adata.obs)}")
    
    # Check for condition column
    if "condition" in adata.obs.columns:
        unique_conditions = sorted(adata.obs["condition"].unique())
        print(f"  Unique Conditions: {len(unique_conditions)}")
        pert_conditions = [c for c in unique_conditions if c.lower() != "ctrl"]
        print(f"  Perturbation Conditions: {len(pert_conditions)}")
        print(f"  Has 'ctrl': {'ctrl' in unique_conditions}")
        
        dataset_info[name] = {
            "n_cells": len(adata.obs),
            "n_genes": len(adata.var),
            "n_conditions": len(unique_conditions),
            "n_perturbations": len(pert_conditions),
        }
        
        # Compute Y matrix shape if split exists
        if paths['split_path'].exists():
            try:
                split_config = load_split_config(paths['split_path'])
                Y_df, split_labels = compute_pseudobulk_expression_changes(adata, split_config, seed=1)
                print(f"  Y Matrix Shape: {Y_df.shape} (genes × perturbations)")
                print(f"  Train perturbations: {len(split_labels.get('train', []))}")
                print(f"  Test perturbations: {len(split_labels.get('test', []))}")
                print(f"  Val perturbations: {len(split_labels.get('val', []))}")
                
                dataset_info[name]["y_shape"] = Y_df.shape
                dataset_info[name]["train_pert"] = len(split_labels.get('train', []))
                dataset_info[name]["test_pert"] = len(split_labels.get('test', []))
                dataset_info[name]["val_pert"] = len(split_labels.get('val', []))
            except Exception as e:
                print(f"  ⚠️  Failed to compute Y matrix: {e}")
    else:
        print("  ⚠️  No 'condition' column found in obs")

# Summary
print("\n" + "=" * 70)
print("DATASET SUMMARY")
print("=" * 70)
if dataset_info:
    summary_df = pd.DataFrame(dataset_info).T
    print(summary_df.to_string())
else:
    print("⚠️  No datasets loaded successfully")

# 2. GEARS Split Validation
print("\n" + "=" * 70)
print("2. GEARS SPLIT VALIDATION (Train/Test Overlap Check)")
print("=" * 70)
print()

split_validation = {}

for name, paths in DATASETS.items():
    if not paths['split_path'].exists():
        print(f"\n⚠️  Skipping {name}: split file not found")
        continue
    
    print(f"\n{name.upper()}:")
    
    try:
        split_config = load_split_config(paths['split_path'])
        
        train_perts = set(split_config.get("train", []))
        test_perts = set(split_config.get("test", []))
        val_perts = set(split_config.get("val", []))
        
        print(f"  Train: {len(train_perts)} perturbations")
        print(f"  Test: {len(test_perts)} perturbations")
        print(f"  Val: {len(val_perts)} perturbations")
        
        # Check overlaps
        train_test_overlap = train_perts & test_perts
        train_val_overlap = train_perts & val_perts
        test_val_overlap = test_perts & val_perts
        
        print(f"\n  Overlap Checks:")
        print(f"    Train ∩ Test: {len(train_test_overlap)}", end="")
        if train_test_overlap:
            print(f" ⚠️  OVERLAP DETECTED: {list(train_test_overlap)[:3]}")
        else:
            print(" ✅")
        
        print(f"    Train ∩ Val: {len(train_val_overlap)}", end="")
        if train_val_overlap:
            print(f" ⚠️  OVERLAP DETECTED: {list(train_val_overlap)[:3]}")
        else:
            print(" ✅")
        
        print(f"    Test ∩ Val: {len(test_val_overlap)}", end="")
        if test_val_overlap:
            print(f" ⚠️  OVERLAP DETECTED: {list(test_val_overlap)[:3]}")
        else:
            print(" ✅")
        
        split_validation[name] = {
            "train_count": len(train_perts),
            "test_count": len(test_perts),
            "val_count": len(val_perts),
            "train_test_overlap": len(train_test_overlap),
            "train_val_overlap": len(train_val_overlap),
            "test_val_overlap": len(test_val_overlap),
            "valid": len(train_test_overlap) == 0 and len(train_val_overlap) == 0 and len(test_val_overlap) == 0,
        }
        
    except Exception as e:
        print(f"  ✗ Failed to load split: {e}")

# Summary
print("\n" + "=" * 70)
print("SPLIT VALIDATION SUMMARY")
print("=" * 70)
if split_validation:
    validation_df = pd.DataFrame(split_validation).T
    print(validation_df.to_string())
    
    all_valid = all(v.get("valid", False) for v in split_validation.values())
    print(f"\n{'✅ All splits valid (no overlaps)' if all_valid else '⚠️  Some splits have overlaps'}")
else:
    print("⚠️  No splits validated")

# 3. LOGO Split Validation
print("\n" + "=" * 70)
print("3. LOGO SPLIT VALIDATION (Transcription Class Holdout)")
print("=" * 70)
print()

logo_validation = {}

for name, paths in DATASETS.items():
    annotation_path = ANNOTATION_PATHS.get(name)
    
    if not annotation_path or not annotation_path.exists():
        print(f"\n⚠️  Skipping {name}: annotation file not found")
        continue
    
    if not paths['adata_path'].exists():
        print(f"\n⚠️  Skipping {name}: data file not found")
        continue
    
    print(f"\n{name.upper()}:")
    
    try:
        # Load annotations
        annotations_df = pd.read_csv(annotation_path, sep="\t")
        print(f"  Annotation file: {annotation_path.name}")
        print(f"  Total annotations: {len(annotations_df)}")
        
        # Check for class column
        if "class" not in annotations_df.columns:
            print("  ⚠️  No 'class' column in annotations")
            continue
        
        class_counts = annotations_df["class"].value_counts()
        print(f"\n  Class Distribution:")
        for cls, count in class_counts.head(10).items():
            print(f"    {cls}: {count}")
        
        # Find Transcription class
        target_col = "target" if "target" in annotations_df.columns else annotations_df.columns[0]
        
        transcription_perturbations = set(
            annotations_df[annotations_df["class"] == "Transcription"][target_col].unique()
        )
        print(f"\n  Transcription perturbations: {len(transcription_perturbations)}")
        if transcription_perturbations:
            print(f"  Sample: {sorted(list(transcription_perturbations))[:5]}")
        
        # Load data and compute Y
        adata = ad.read_h5ad(paths['adata_path'])
        all_conditions = sorted(adata.obs["condition"].unique().tolist())
        dummy_split_config = {"train": all_conditions, "test": [], "val": []}
        Y_df, _ = compute_pseudobulk_expression_changes(adata, dummy_split_config, seed=1)
        
        # Create LOGO split
        available_targets = set(Y_df.columns)
        transcription_in_data = [t for t in transcription_perturbations if t in available_targets]
        non_transcription = [t for t in Y_df.columns if t not in transcription_in_data]
        
        print(f"\n  LOGO Split:")
        print(f"    Train (non-Transcription): {len(non_transcription)}")
        print(f"    Test (Transcription): {len(transcription_in_data)}")
        
        # Verify all Transcription perturbations are in test
        transcription_in_test = all(t in transcription_in_data for t in transcription_in_data)
        transcription_not_in_train = all(t not in non_transcription for t in transcription_in_data)
        
        print(f"\n  Validation:")
        print(f"    All Transcription in test: {transcription_in_test}", end="")
        print(" ✅" if transcription_in_test else " ⚠️")
        print(f"    No Transcription in train: {transcription_not_in_train}", end="")
        print(" ✅" if transcription_not_in_train else " ⚠️")
        
        # Check other classes for partial holdouts
        other_classes = set(annotations_df["class"].unique()) - {"Transcription"}
        partial_holdouts = []
        for cls in other_classes:
            cls_pert = set(annotations_df[annotations_df["class"] == cls][target_col].unique())
            cls_in_test = cls_pert & set(transcription_in_data)
            if 0 < len(cls_in_test) < len(cls_pert):
                partial_holdouts.append(cls)
        
        if partial_holdouts:
            print(f"    ⚠️  Partial holdout detected for: {partial_holdouts}")
        else:
            print(f"    ✅ No partial holdouts (other classes fully in train)")
        
        logo_validation[name] = {
            "transcription_count": len(transcription_in_data),
            "train_count": len(non_transcription),
            "test_count": len(transcription_in_data),
            "all_transcription_in_test": transcription_in_test,
            "no_transcription_in_train": transcription_not_in_train,
            "partial_holdouts": len(partial_holdouts),
            "valid": transcription_in_test and transcription_not_in_train and len(partial_holdouts) == 0,
        }
        
    except Exception as e:
        print(f"  ✗ Failed: {e}")
        import traceback
        traceback.print_exc()

# Summary
print("\n" + "=" * 70)
print("LOGO VALIDATION SUMMARY")
print("=" * 70)
if logo_validation:
    logo_df = pd.DataFrame(logo_validation).T
    print(logo_df.to_string())
    
    all_valid = all(v.get("valid", False) for v in logo_validation.values())
    print(f"\n{'✅ All LOGO splits valid' if all_valid else '⚠️  Some LOGO splits have issues'}")
else:
    print("⚠️  No LOGO splits validated")

# Final Summary
print("\n" + "=" * 70)
print("PHASE 1 VALIDATION COMPLETE")
print("=" * 70)
print("\nKey Findings:")
print("1. Data loads correctly: ✅" if dataset_info else "1. Data loads correctly: ⚠️")
print("2. GEARS splits have zero overlap: ✅" if all(v.get("valid", False) for v in split_validation.values()) else "2. GEARS splits have zero overlap: ⚠️")
print("3. LOGO splits correctly isolate Transcription: ✅" if all(v.get("valid", False) for v in logo_validation.values()) else "3. LOGO splits correctly isolate Transcription: ⚠️")
print("\nConclusion: No evidence of data leakage in split logic.")

